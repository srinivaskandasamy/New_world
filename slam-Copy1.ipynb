{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import loadtxt, pi\n",
    "path = loadtxt(r\"C:\\Users\\Srinivas\\Documents\\GitHub\\New_world\\robot_trajectory.txt\")\n",
    "poses = []\n",
    "for i in range(len(path)):\n",
    "    if path[i][3]!=0:\n",
    "        if path[i][3] == 1:\n",
    "            poses.append([path[i][0],path[i][1],path[i-1][2],pi])\n",
    "        elif path[i][3] == 3:\n",
    "            poses.append([path[i][0],path[i][1],path[i-1][2],0])\n",
    "        elif path[i][3] == 2:\n",
    "            poses.append([path[i][0],path[i][1],path[i-1][2],3*pi/2])\n",
    "        elif path[i][3] == 4:\n",
    "            poses.append([path[i][0],path[i][1],path[i-1][2],pi/2])\n",
    "\n",
    "lposes = len(poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from math import tan, atan, sqrt, cos, sin\n",
    "from numpy import eye, diag, array, exp, ceil, floor, dot, arange\n",
    "from numpy.linalg import inv, det\n",
    "from random import gauss, randint, uniform\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "class Particle:\n",
    "    def __init__(self,pose):\n",
    "        self.pose = pose\n",
    "        self.landmark_poses = []\n",
    "        self.landmark_covariances = []\n",
    "        self.wall_list = [] # Stores the histogram from the tree w\n",
    "        self.var = 0\n",
    "        self.length = 480\n",
    "#         self.w = [['0'],['90'],['180'],['270']] # Landmark as histograms stored by each particle\n",
    "        self.w = []\n",
    "    \n",
    "    def number_of_landmarks(self):\n",
    "        return len(self.w)\n",
    "    \n",
    "    @staticmethod\n",
    "    def motion(pose, control, WB, dt):  \n",
    "        angle = (control[1]+pose[2])%(2*pi)\n",
    "        return ([x+y for x,y in zip(pose,[control[0]*dt*cos(angle),\n",
    "                                          control[0]*dt*sin(angle),\n",
    "                                          control[0]*dt*sin(control[1])/WB])])\n",
    "    \n",
    "    def move(self,control,WB,dt,pose):\n",
    "        self.pose = pose\n",
    "        \n",
    "    def expected_measurement(self,pose,landmark):   # Not considering the other component  \n",
    "        return array([landmark[0],tan(landmark[1]-pose[2])])      \n",
    "    \n",
    "    def measurement_correspondence(self,pose,measurement,number_of_landmarks,Qt_measurement_covariance):\n",
    "        likelihoods = []\n",
    "        for i in range(number_of_landmarks):\n",
    "            likelihoods.append(self.landmark_correspondence(measurement,i,Qt_measurement_covariance))\n",
    "        return likelihoods\n",
    "    \n",
    "    @staticmethod\n",
    "    def dh_dlandmark(pose,landmark):\n",
    "        return array([[1,0],[0,-1/(cos(pose[2]-landmark[2])**2)]]) # H - Direct odometry measurement and collision\n",
    "    \n",
    "    def H_and_Ql_from_measurement(self,landmark,Qt_measurement_covariance,i):\n",
    "        \n",
    "        H = self.dh_dlandmark(self.pose,landmark)\n",
    "        Ql = dot(dot(H,self.landmark_covariances[i]),H.T) + Qt_measurement_covariance\n",
    "        \n",
    "        return (H, Ql)\n",
    "        \n",
    "    def landmark_correspondence_likelihood(self,measurement,landmark_number,Qt):\n",
    "        # For a given measurment and a landmark number, it returns a suitable likelihood value of the correspondence\n",
    "\n",
    "        landmark = self.w[landmark_number]\n",
    "        num_wall = len(landmark)-1\n",
    "        likelihood=[]\n",
    "        for i in arange(1,num_wall,1):\n",
    "            \n",
    "            phi = landmark[i][1]\n",
    "            theta = self.pose[2]\n",
    "\n",
    "            # Prune the landmark which are not in the robot's frame of collision\n",
    "            if phi == pi/2 * floor(theta/(pi/2)) or phi == pi/2 * ceil(theta/(pi/2)): \n",
    "                if landmark[i][1] % pi == 0:\n",
    "                    # Check the x-value correspondence\n",
    "                    zhat = self.expected_measurement(self.pose,landmark[i])\n",
    "                    H, Ql = self.H_and_Ql_from_measurement(landmark[i],diag([Qt[0][0],Qt[2][2]]),landmark_number)\n",
    "                    dz = array([measurement[0],measurement[2]])-zhat\n",
    "                else:\n",
    "                    # Check the y-value correspondence\n",
    "                    zhat = self.expected_measurement(self.pose,landmark[i])\n",
    "                    H, Ql = self.H_and_Ql_from_measurement(landmark[i],diag([Qt[1][1],Qt[2][2]]),landmark_number)\n",
    "                    dz = array([measurement[1],measurement[2]])-zhat\n",
    "                \n",
    "                # Compute likelihood\n",
    "                sqrtdetQl = sqrt(det(Ql))\n",
    "                normal = 1 / (2*pi*sqrtdetQl)\n",
    "                l = normal * exp(-0.5*dot(dot(dz.T,inv(Ql)),dz))            \n",
    "            else: \n",
    "                l = 0\n",
    "            likelihood.append(l)\n",
    "        return max(likelihood)\n",
    "    \n",
    "    def dis_Bayes(self,wall_num, index, index_sttdev): # The landmark, the position of measurement of \n",
    "        # that landmark and stddev(covariance)\n",
    "        \n",
    "        for i in range(index_stddev):\n",
    "            # Corridor width=120, Standard deviation=11, Creating a triangular distribution of width=2sigma\n",
    "            b=1.5-i*(0.5/index_stddev) \n",
    "            if j-i >= 0: \n",
    "                self.wall_list[wall_num][index-i]*=b\n",
    "            if j+i <=l-1:\n",
    "                self.wall_list[wall_num][index+i]*=b\n",
    "\n",
    "            self.wall_list[wall_num] = [i*5/sum(wall_list[wall_num]) for i in a] # Scaling parameter=5\n",
    "    \n",
    "    def initialize_new_landmark(self,measurement,Qt, l):\n",
    "        # Measurement to be modified (Orientations to 90 degrees)\n",
    "        self.w.insert(self.var,[self.var])        \n",
    "        orientation = self.pose[2]+atan(measurement[2])\n",
    "        \n",
    "        if orientation % pi == 0: # Need to encode x and theta component in Sigma\n",
    "            self.w[self.var].append([poses[0], orientation])\n",
    "            self.wall_list.append([1/l]*l)\n",
    "            self.dis_Bayes(self.var,int(poses[1]),20)\n",
    "            \n",
    "            Hinv = array([[1,0],[0,1/(1+measurement[2]**2)]])\n",
    "            Qtx = np.diag([Qt[0][0],Qt[2][2]])\n",
    "            Sigma = dot(dot(Hinv,Qtx),Hinv.T)\n",
    "            self.landmark_covariances.append(Sigma)\n",
    "        else:\n",
    "            self.w[self.var].append([poses[1], orientation])\n",
    "            self.wall_list.append([1/l]*l)\n",
    "            self.dis_Bayes(self.var,int(poses[0]),20)   \n",
    "            \n",
    "            Hinv = array([[1,0],[0,1/(1+measurement[2]**2)]])\n",
    "            Qty = np.diag([Qt[1][1],Qt[2][2]]) # y component and orientation\n",
    "            Sigma = dot(dot(Hinv,Qty),Hinv.T)\n",
    "            self.landmark_covariances.append(Sigma)\n",
    "            \n",
    "        self.var = self.var + 1\n",
    "        \n",
    "    def update_landmark(self,measurement,landmark_number,Qt_measurement_covariance):\n",
    "        \n",
    "        for i in arange(1,len(self.w[landmark_number]),1):\n",
    "            mu = self.w[landmark_number][i][0]\n",
    "            Sigma = self.landmark_covariances[landmark_number]\n",
    "            H, Ql = self.H_and_Ql_from_measurement(self.w[landmark_number][i],Qt_measurement_covariance,landmark_number)\n",
    "        K = dot(dot(Sigma,H.T),inv(Ql))\n",
    "        \n",
    "        mu = mu + dot(K,(measurement - self.expected_measurement(self.pose,mu)))\n",
    "        Sigma = dot((eye(3) - K*H),Sigma)\n",
    "        \n",
    "        return (mu,Sigma)\n",
    "    \n",
    "    def update_particle(self,measurement,number_of_landmarks,minimum_correspondence_likelihood,Qt_measurement_covariance):\n",
    "        likelihoods = []\n",
    "        for i in range(number_of_landmarks):\n",
    "            likelihoods.append(self.landmark_correspondence_likelihood(measurement,i,Qt_measurement_covariance))\n",
    "\n",
    "        if not likelihoods or max(likelihoods) < minimum_correspondence_likelihood:\n",
    "            self.initialize_new_landmark(measurement,Qt_measurement_covariance, self.length)\n",
    "            return minimum_correspondence_likelihood\n",
    "        \n",
    "        else:\n",
    "            lmax = max(likelihoods)\n",
    "            lmax_index = likelihoods.index(lmax)\n",
    "            mu,Sigma = self.update_landmark(measurement,lmax_index,Qt_measurement_covariance)\n",
    "            self.landmark_poses[lmax_index] = mu\n",
    "            self.landmark_covariances[lmax_index] = Sigma\n",
    "            \n",
    "            return lmax\n",
    "\n",
    "class FastSLAM:\n",
    "    def __init__(self,initial_particles,robot_width,minimum_correspondence_likelihood,measurement_stddev,\n",
    "                 x_stddev,y_stddev,control_speed_factor,control_head_factor, sample_time):\n",
    "        # Particles\n",
    "        self.particles = initial_particles\n",
    "        \n",
    "        # Constants\n",
    "        self.robot_width = robot_width\n",
    "        self.minimum_correspondence_likelihood = minimum_correspondence_likelihood\n",
    "        self.xstddev = xstddev\n",
    "        self.ystddev = ystddev\n",
    "        self.measurement_stddev = measurement_stddev\n",
    "        self.control_speed_factor = control_speed_factor\n",
    "        self.control_head_factor = control_head_factor\n",
    "        self.dt = sample_time\n",
    "        self.WB = robot_width\n",
    "        \n",
    "    def predict(self,pose,control):\n",
    "        # Prediction step of FastSLAM\n",
    "\n",
    "        speed, head = control\n",
    "        speed_std = self.control_speed_factor * sqrt(speed) # To be modified\n",
    "        head_std  = self.control_head_factor * sqrt(359 - head) # To be modified; mirror image of the speed deviation\n",
    "        \n",
    "        for p in self.particles:\n",
    "            speed = gauss(speed,speed_std)\n",
    "            head = gauss(head,head_std)\n",
    "            p.move([speed,head],self.WB,self.dt,pose)\n",
    "    \n",
    "    def update_and_compute_weights(self,measurement):\n",
    "\n",
    "        Qt_measurement_covariance = diag([self.xstddev ** 2, self.ystddev ** 2,self.measurement_stddev ** 2]) \n",
    "        weights = []\n",
    "        for p in self.particles:\n",
    "            number_of_landmarks = p.number_of_landmarks()\n",
    "            weight = p.update_particle(measurement,number_of_landmarks,self.minimum_correspondence_likelihood,\n",
    "                                            Qt_measurement_covariance)\n",
    "            weights.append(weight)\n",
    "            \n",
    "        return weights\n",
    "    \n",
    "    def resample(self,weights):\n",
    "        \n",
    "        new_particles = []\n",
    "        max_weight = max(weights)\n",
    "        index = randint(0, len(self.particles)-1)\n",
    "        offset = 0.0\n",
    "        for i in xrange(len(self.particles)):\n",
    "            offset += uniform(0, 2.0 * max_weight)\n",
    "            while offset > weights[index]:\n",
    "                offset -= weights[index]\n",
    "                index = (index + 1) % len(weights)\n",
    "            new_particles.append(copy.deepcopy(self.particles[index]))\n",
    "            \n",
    "        return new_particles\n",
    "    \n",
    "    def correct(self,measurement):\n",
    "        \n",
    "        weights = self.update_and_compute_weights(measurement)\n",
    "        self.particles = self.resample(weights)\n",
    "        \n",
    "        return weights\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    robot_width = 2\n",
    "    sample_time = 0.01\n",
    "    \n",
    "    minimum_correspondence_likelihood = 1e-3\n",
    "    xstddev = 0.001\n",
    "    ystddev = 0.001\n",
    "    measurement_stddev = 0.001\n",
    "    \n",
    "    control_speed_factor = 0.01\n",
    "    control_head_factor = 0.01\n",
    "    number_of_particles = 10\n",
    "    \n",
    "    start_state = poses[0][0:3]\n",
    "    initial_particles = [copy.copy(Particle(start_state))\n",
    "                         for _ in xrange(number_of_particles)]\n",
    "\n",
    "    fs = FastSLAM(initial_particles,robot_width,minimum_correspondence_likelihood,measurement_stddev,xstddev,ystddev,\n",
    "                 control_speed_factor,control_head_factor, sample_time)\n",
    "    \n",
    "    for i in xrange(lposes-1):\n",
    "        # Correction step\n",
    "        print \"Iteration\",i\n",
    "        '''Use of entire trajectory for the correction step (Rao-Blackwellization)''' \n",
    "        w = fs.correct(array([poses[i][0],poses[i][1],tan(poses[i][3]-poses[i][2])])) # Odometry and collision values\n",
    "        #Prediction\n",
    "        fs.predict(poses[i+1][0:3],[100,poses[i+1][2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "from IPython import display\n",
    "\n",
    "xl = []\n",
    "yl = []\n",
    "xt = []\n",
    "yt = []\n",
    "\n",
    "for i in xrange(len(fs.particles[0].landmark_poses)):\n",
    "    xl.append(fs.particles[0].landmark_poses[i][0])\n",
    "    yl.append(fs.particles[0].landmark_poses[i][1])\n",
    "    xt.append(poses[i][0])\n",
    "    yt.append(poses[i][1])\n",
    "\n",
    "plt.plot(xl,yl,'o',color='b')\n",
    "plt.plot(xt,yt,'o',color='r')\n",
    "plt.show()\n",
    "print \"xl is\",xl\n",
    "print \"yl is\",yl\n",
    "print \"xt is\",xt\n",
    "print \"yt is\",yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[111.5, 0]]]\n"
     ]
    }
   ],
   "source": [
    "a=[]\n",
    "f1=[]\n",
    "a.insert(0,f1)\n",
    "f1.append([111.5,0])\n",
    "print a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
