{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "path = loadtxt(r\"C:\\Users\\localadmin.TUD0035314\\Documents\\GitHub\\New_world\\robot_trajectory.txt\")\n",
    "\n",
    "# Lets prune the information in our hand since not information is gained from the path in between the collisions\n",
    "# Only the poses of collision are needed, that define an event based system. The control input should act on the collision pose\n",
    "# and see if possible if the entire pose trajectory is same as the list of landmarks (even thought they start to fuse in \n",
    "# formation of hyperplanes or half-spaces)\n",
    "poses = []\n",
    "poses.append(path[0])\n",
    "for i in range(len(path)):\n",
    "    if path[i][3]!=0:\n",
    "#         poses.append(path[i-1])\n",
    "        poses.append(path[i]) \n",
    "#         poses.append(path[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import tan, atan, sqrt, sec\n",
    "from numpy import eye\n",
    "from random import gauss, randint, uniform\n",
    "import copy\n",
    "\n",
    "# Creating Particle class, where each particle is an instantiation of this class\n",
    "class Particle:\n",
    "    def __init__(self,pose):\n",
    "        self.pose = pose\n",
    "        self.landmark_poses = [] # (list of x,y,theta) or (list of x,y,a,b) where aTx<=b where (theta, x or y) in the first \n",
    "        # option define the hyperplane aTx<=b\n",
    "        self.landmark_covariances = [] # list of 3x3 matrices - If we go for a 1D covariance as in GUI, we consider only the \n",
    "        # correspoding axis covariances (x or y)\n",
    "        \n",
    "    def number_of_landmarks(self):\n",
    "        # Returns the number of landmarks for a particle and also the number of collisions to define a hyperplane or half-space \n",
    "        # in future\n",
    "        return len(self.landmark_poses)\n",
    "    \n",
    "    @staticmenthod\n",
    "    def motion(pose, control, WB, dt):  # Unicycle motion model\n",
    "        control[1]=control[1]%360\n",
    "        angle = ((control[1]+pose[2]*180/pi)%360)*pi/180\n",
    "        return ([x+y for x,y in zip(pose,[control[0]*dt*cos(angle), \n",
    "                                          control[0]*dt*sin(angle), \n",
    "                                          control[0]*dt*sin(control[1]*pi/180)/WB])])\n",
    "    \n",
    "    def move(self,control,WB,dt):\n",
    "        # Given the control, move the robot in a direction\n",
    "        self.pose = self.motion(self.pose,control,WB,dt)\n",
    "        \n",
    "    def expected_measurement(pose,landmark): # Pose in the frame of robot and landmark orientation in the world frame \n",
    "        # This function is called only if the landmark list is non-empty\n",
    "        # The position and orientation of the landmark is defined in the world-coordinate frame and not in the robot frame of \n",
    "        # orientation.\n",
    "        theta = pose[2]\n",
    "        phi = landmark[2]\n",
    "        \n",
    "        return tan(phi-theta) # Ratio of the accelerations ax/ay       \n",
    "    \n",
    "    def measurement_correspondence(self,pose,measurement,number_of_landmarks,Qt_measurment_covariance):\n",
    "        # For a given measurment, returns a set of correspondences over the list of available landmarks\n",
    "        likelihoods = []\n",
    "        for i in range(number_of_landmarks):\n",
    "            # Measurement  - Ratio of ax/ay\n",
    "            likelihoods.append(self.landmark_correspondence(measurement,i,Qt_measurment_covariance))\n",
    "        return likelihoods\n",
    "    \n",
    "    @staticmethod\n",
    "    def dh_dlandmark(pose,landmark):\n",
    "        theta = pose[2]\n",
    "        phi = landmark[2]\n",
    "        # H = Derivative of self.expected_measurement with respect to the landmark at the point of pose and landmark \n",
    "        # H = 1x3 where there is one output and three landmark variables x,y,phi\n",
    "        H = [0, 0, -sec(theta-phi)**2 ] # Odometry measurement not taken into account\n",
    "        \n",
    "        return H\n",
    "        \n",
    "    def landmark_correspondence_likelihood(self,measurement,landmark_number,Qt_measurement_covariance):\n",
    "        # For a given measurment and a landmark number, it returns a suitable likelihood value of the correspondence\n",
    "\n",
    "        landmark = self.landmark_poses[landmark_number]\n",
    "        phi = landmark[2]\n",
    "        theta = self.pose[2]\n",
    "        \n",
    "        # Prune the landmark which are not in the robot's frame of collision\n",
    "        if phi == 90 * floor(theta/90) or phi == 90 * ceil(theta/90): \n",
    "            \n",
    "            zhat = self.expected_measurement(self.pose,landmark)\n",
    "            \n",
    "            # Computing the derivative of measurement function with respect to the landmark variables\n",
    "            H = dh_dlandmark(self.pose,landmark)\n",
    "            # Computing the variance propagation of landmarks and the measurement\n",
    "            Q = H * self.landmark_covariances[landmark_number] * H.T + Qt_measurement_covariance\n",
    "            \n",
    "            dz = measurement - zhat\n",
    "            \n",
    "            # Computing (log) likelihood\n",
    "            # l = \n",
    "            \n",
    "            # Now the orientations have been checked. Check whether they lie close to each other\n",
    "            # If l is higher than the threshold, then lets compute the threshold whether they lie close to each other with the \n",
    "            # same orientation. Later, we have to incorporate the ones with the corners so that they can form halfspaces\n",
    "            # Computing the likelihood along the axis of interest for close relationship to fuse if needed.\n",
    "            \n",
    "            \n",
    "            # Create a ball of confidence thresholds for the robot's point of collision with the threshold equal to corridor width\n",
    "            # If less than threshold, plan to fuse them in future by setting a higher likelihood\n",
    "            \n",
    "        else: \n",
    "            l = 0\n",
    "        \n",
    "        # If less than the corridor width threshold, the likelihood is higher\n",
    "        # If this is executed, create a flag to fuse the measurement and feature to get a bigger feature\n",
    "        # Ultimately, a hyperplane or half-space is created, which forms a comaponent of planning for verification\n",
    "        \n",
    "        # If more, then initialize a new landmark\n",
    "        \n",
    "        return l\n",
    "    \n",
    "    def intialize_new_landmark(self,measurement,Qt_measurement_covariance):\n",
    "        # Take care of the orientation, coordinate transformation from the robot frame to the world frame\n",
    "        self.landmark_poses.append([self.pose[0],self.pose[1],self.pose[2]+atan(measurement)]) # Orientation update as FSM\n",
    "        # We have to modify the orientation of the landmark to multiples of 90 degrees\n",
    "        Hinv = np.array([[1,0,0],[0,1,0],[0,0,1/(1+measurement**2)]])\n",
    "        Sigma = Hinv*Qt_measurement_covariance*\n",
    "        self.landmark_covariances.append(Hinv*Sigma*Hinv.T) # Remember to add the covariance of the particle at that moment\n",
    "        \n",
    "    def update_landmark(self,measurment,landmark_number,Qt_measurement_covariance):\n",
    "        # Updated landmark's estimated position and covariance using the correspondence variable\n",
    "        # Fuse the landmark with measurements if the flag in the previous landmark_correspondence is True\n",
    "        # After Fusion, revise the landmark set \n",
    "        # Define a hyperplane/halfspace to define a higher level feature set\n",
    "        # The landmarks are defined as histograms rather than gaussian or particles\n",
    "        pass\n",
    "    \n",
    "    def update_particle(self,measurement,number_of_landmarks,minimum_correspondence_likelihood,Qt_measurement_covariance):\n",
    "        # This is the ultimate function for updating the particle filter\n",
    "        \n",
    "        # Compute the likelihood of the measurment to the list of number of landmarks - Correspondence calculation\n",
    "        \n",
    "        # If the likelihoods list is empty or the correspondence obtained for all the landmarks is less than the \n",
    "        # minimum_correspondence_likelihood, add a new landmark\n",
    "        \n",
    "        likelihoods = []\n",
    "        if not likelihoods or max(likelihoods) < minimum_correspondence_likelihood:\n",
    "            # Adding a new landmark\n",
    "            return minimum_correspondence_likelihood\n",
    "        \n",
    "        # Else, update the landmark with the best likelihood correspondence and update thhe list to a higher feature set level,\n",
    "        # which runs parallel by creating hyperplanes/halfspaces and planning thereby\n",
    "        else:\n",
    "            # Landmark update using EKF or UKF after computing the (max,argmax) of measurement_likelihoods\n",
    "            # Find w, maximum likelihood and correspondence landmark index (indices for creating a hyperplane)\n",
    "            \n",
    "            return w\n",
    "\n",
    "class FastSLAM:\n",
    "    def __init__(self,intial_particles,robot_width,minimum_correspondence_likelihood,measurement_stddev,\n",
    "                 control_speed_factor,control_head_factor, sample_time,robot_width):\n",
    "        # Particles\n",
    "        self.particles = intial_particles\n",
    "        \n",
    "        # Constants\n",
    "        self.robot_width = robot_width\n",
    "        self.minimum_correspondence_likelihood = minimum_correspondence_likelihood\n",
    "        self.measurement_stddev = measurement_stddev\n",
    "        self.control_speed_factor = control_speed_factor\n",
    "        self.control_head_factor = control_head_factor\n",
    "        self.dt = sample_time\n",
    "        self.WB = robot_width\n",
    "        \n",
    "    def predict(self,control):\n",
    "        # Prediction step of FastSLAM\n",
    "        speed, head = control\n",
    "        \n",
    "        # Parabolic or 1D quadratic fit to the standard deviation of speed \n",
    "        speed_std = self.control_speed_factor * sqrt(speed) # To be modified\n",
    "        head_std  = self.control_head_factor * sqrt(359 - head) # To be modified; mirror image of the speed deviation\n",
    "        \n",
    "        for p in self.particles:\n",
    "            speed = gauss(speed,speed_std)\n",
    "            head = gauss(head,head_std)\n",
    "            p.move((speed,head),self.WB,self.dt)\n",
    "    \n",
    "    def update_and_compute_weights(self,landmarks):\n",
    "        \n",
    "        Qt_measurement_covariance = self.measurement_stddev ** 2 # Measurement covariance for the ratio\n",
    "        \n",
    "        weights = []\n",
    "        \n",
    "        for p in self.particles:\n",
    "            \n",
    "            number_of_landmarks = p.number_of_landmarks()\n",
    "            weight = 1\n",
    "            \n",
    "            for measurement in landmarks:\n",
    "                # Cumulative product of weights for a particle based on the measurements\n",
    "                weight *= p.update_particle(measurement,number_of_landmarks,self.minimum_correspondence_likelihood,\n",
    "                                            Qt_measurement_covariance)\n",
    "                weights.append(weight)\n",
    "                \n",
    "            weights.append(weight)\n",
    "            \n",
    "        return weights\n",
    "    \n",
    "    def resample(self,weights):\n",
    "        # Returns a new set of resampled particles, proportional to their weight\n",
    "        new_particles = []\n",
    "        max_weight = max(weights)\n",
    "        index = randint(0, len(self.particles)-1)\n",
    "        offset = 0.0\n",
    "        for i in xrange(len(self.particles)):\n",
    "            offset += uniform(0, 2.0 * max_weight)\n",
    "            while offset > weights[index]:\n",
    "                offset -= weights[index]\n",
    "                index = (index + 1) % len(weights)\n",
    "            new_particles.append(copy.deepcopy(self.particles[index]))\n",
    "            \n",
    "        return new_particles\n",
    "        \n",
    "    \n",
    "    def correct(landmarks):\n",
    "        # Correction step of FastSLAM\n",
    "        weights = self.update_and_compute_weights(landmarks)\n",
    "        self.particles = self.resample(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3]\n",
      " [2 4]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.array([[1, 2],[3, 4]])\n",
    "print A.T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
