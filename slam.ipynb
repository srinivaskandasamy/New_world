{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "path = loadtxt(r\"C:\\Users\\localadmin.TUD0035314\\Documents\\GitHub\\New_world\\robot_trajectory.txt\")\n",
    "\n",
    "# Lets prune the information in our hand since not information is gained from the path in between the collisions\n",
    "poses = []\n",
    "poses.append(path[0])\n",
    "for i in range(len(path)):\n",
    "    if path[i][3]!=0:\n",
    "        poses.append(path[i-1])\n",
    "        poses.append(path[i])\n",
    "        poses.append(path[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import tan, atan, sqrt\n",
    "from numpy import eye\n",
    "from random import gauss, randint, uniform\n",
    "import copy\n",
    "\n",
    "# Creating Particle class, where each particle is an instantiation of this class\n",
    "class Particle:\n",
    "    def __init__(self,pose):\n",
    "        self.pose = pose\n",
    "        self.landmark_poses = [] # (list of x,y,theta) or (list of x,y,a,b) where aTx<=b where (theta, x or y) in the first \n",
    "        # option define the hyperplane aTx<=b\n",
    "        self.landmark_covariances = [] # list of 3x3 matrices\n",
    "        \n",
    "    def number_of_landmarks(self):\n",
    "        # Returns the number of landmarks for a particle and also the number of collisions to define a hyperplane or half-space \n",
    "        # in future\n",
    "        return len(self.landmark_poses)\n",
    "    \n",
    "    @staticmenthod\n",
    "    def motion(pose, control, WB, dt):  # Unicycle motion model\n",
    "        control[1]=control[1]%360\n",
    "        angle = ((control[1]+pose[2]*180/pi)%360)*pi/180\n",
    "        return ([x+y for x,y in zip(pose,[control[0]*dt*cos(angle), \n",
    "                                          control[0]*dt*sin(angle), \n",
    "                                          control[0]*dt*sin(control[1]*pi/180)/WB])])\n",
    "    \n",
    "    def move(self,control,WB,dt):\n",
    "        # Given the control, move the robot in a direction\n",
    "        self.pose = self.motion(self.pose,control,WB,dt)\n",
    "        \n",
    "    def measurement(pose,landmark):\n",
    "        # Take from the actual code in Sphero\n",
    "        return tan(pose[2]-landmark[2]) # Ratio of the accelerations ax/ay\n",
    "    \n",
    "    def measurement_correspondence(self,pose,measurement,number_of_landmarks,Qt_measurment_covariance):\n",
    "        # For a given measurment, returns a set of correspondences over the list of available landmarks\n",
    "        likelihoods = []\n",
    "        for i in range(number_of_landmarks):\n",
    "            likelihoods.append(self.landmark_correspondence(measurement,i,Qt_measurment_covariance))\n",
    "        return likelihoods\n",
    "    \n",
    "    def landmark_correspondence(self,measurement,landmark_number,Qt_measurement_covariance):\n",
    "        # For a given measurment and a landmark number, it returns a suitable likelihood value of the correspondence\n",
    "        \n",
    "        # If less than the corridor width threshold, the likelihood is higher\n",
    "        # If this is executed, create a flag to fuse the measurement and feature to get a bigger feature\n",
    "        # Ultimately, a hyperplane or half-space is created, which forms a comaponent of planning for verification\n",
    "        \n",
    "        # If more, then initialize a new landmark\n",
    "        \n",
    "        return 1\n",
    "    \n",
    "    def intialize_new_landmark(self,measurement,Qt_measurement_covariance):\n",
    "        # Take care of the orientation, coordinate transformation from the robot frame to the world frame\n",
    "        self.landmark_poses.append([self.pose[0],self.pose[1],self.pose[2]+atan(measurement)]) # Orientation update as FSM\n",
    "        self.landmark_covariances.append(eye(3))\n",
    "        \n",
    "    def update_landmark(self,measurment,landmark_number,Qt_measurement_covariance):\n",
    "        # Updated landmark's estimated position and covariance using the correspondence variable\n",
    "        # Fuse the landmark with measurements if the flag in the previous landmark_correspondence is True\n",
    "        # After Fusion, revise the landmark set \n",
    "        # Define a hyperplane/halfspace to define a higher level feature set\n",
    "        pass\n",
    "    \n",
    "    def update_particle(self,measurement,number_of_landmarks,minimum_correspondence_likelihood,Qt_measurement_covariance):\n",
    "        # This is the ultimate function for updating the particle filter\n",
    "        \n",
    "        # Compute the likelihood of the measurment to the list of number of landmarks - Correspondence calculation\n",
    "        \n",
    "        # If the likelihoods list is empty or the correspondence obtained for all the landmarks is less than the \n",
    "        # minimum_correspondence_likelihood, add a new landmark\n",
    "        \n",
    "        likelihoods = []\n",
    "        if not likelihoods or max(likelihoods) < minimum_correspondence_likelihood:\n",
    "            # Adding a new landmark\n",
    "            return minimum_correspondence_likelihood\n",
    "        \n",
    "        # Else, update the landmark with the best likelihood correspondence and update thhe list to a higher feature set level,\n",
    "        # which runs parallel by creating hyperplanes/halfspaces and planning thereby\n",
    "        else:\n",
    "            # Landmark update using EKF or UKF after computing the (max,argmax) of measurement_likelihoods\n",
    "            # Find w, maximum likelihood and correspondence landmark index (indices for creating a hyperplane)\n",
    "            \n",
    "            return w\n",
    "\n",
    "class FastSLAM:\n",
    "    def __init__(self,intial_particles,robot_width,minimum_correspondence_likelihood,measurement_stddev,\n",
    "                 control_speed_factor,control_head_factor, sample_time,robot_width):\n",
    "        # Particles\n",
    "        self.particles = intial_particles\n",
    "        \n",
    "        # Constants\n",
    "        self.robot_width = robot_width\n",
    "        self.minimum_correspondence_likelihood = minimum_correspondence_likelihood\n",
    "        self.measurement_stddev = measurement_stddev\n",
    "        self.control_speed_factor = control_speed_factor\n",
    "        self.control_head_factor = control_head_factor\n",
    "        self.dt = sample_time\n",
    "        self.WB = robot_width\n",
    "        \n",
    "    def predict(self,control):\n",
    "        # Prediction step of FastSLAM\n",
    "        speed, head = control\n",
    "        \n",
    "        # Parabolic or 1D quadratic fit to the standard deviation of speed \n",
    "        speed_std = self.control_speed_factor * sqrt(speed) # To be modified\n",
    "        head_std  = self.control_head_factor * sqrt(359 - head) # To be modified; mirror image of the speed deviation\n",
    "        \n",
    "        for p in self.particles:\n",
    "            speed = gauss(speed,speed_std)\n",
    "            head = gauss(head,head_std)\n",
    "            p.move((speed,head),self.WB,self.dt)\n",
    "    \n",
    "    def update_and_compute_weights(self,landmarks):\n",
    "        \n",
    "        Qt_measurement_covariance = self.measurement_stddev ** 2 # Measurement covariance for the ratio\n",
    "        \n",
    "        weights = []\n",
    "        \n",
    "        for p in self.particles:\n",
    "            \n",
    "            number_of_landmarks = p.number_of_landmarks()\n",
    "            weight = 1\n",
    "            \n",
    "            for measurement in landmarks:\n",
    "                # Cumulative product of weights for a particle based on the measurements\n",
    "                weight *= p.update_particle(measurement,number_of_landmarks,self.minimum_correspondence_likelihood,\n",
    "                                            Qt_measurement_covariance)\n",
    "                weights.append(weight)\n",
    "                \n",
    "            weights.append(weight)\n",
    "            \n",
    "        return weights\n",
    "    \n",
    "    def resample(self,weights):\n",
    "        # Returns a new set of resampled particles, proportional to their weight\n",
    "        new_particles = []\n",
    "        max_weight = max(weights)\n",
    "        index = randint(0, len(self.particles)-1)\n",
    "        offset = 0.0\n",
    "        for i in xrange(len(self.particles)):\n",
    "            offset += uniform(0, 2.0 * max_weight)\n",
    "            while offset > weights[index]:\n",
    "                offset -= weights[index]\n",
    "                index = (index + 1) % len(weights)\n",
    "            new_particles.append(copy.deepcopy(self.particles[index]))\n",
    "            \n",
    "        return new_particles\n",
    "        \n",
    "    \n",
    "    def correct(landmarks):\n",
    "        # Correction step of FastSLAM\n",
    "        weights = self.update_and_compute_weights(landmarks)\n",
    "        self.particles = self.resample(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import gauss, randint"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
